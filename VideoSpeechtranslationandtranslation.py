# -*- coding: utf-8 -*-
"""Copy of Copie_de_Youtube_Video_Transcriptor_.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13oHJtrwIAUGue0MskvIET7IiLdJKao0r

### Installing the requirements
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# 
# !pip install --upgrade yt_dlp
# !pip install pydub SpeechRecognition ffmpeg ffprobe googletrans==3.1.0a0 transformers

import yt_dlp
import time
import re
import os
from pydub import AudioSegment
import speech_recognition as sr
import math
from tqdm import tqdm
from googletrans import Translator
from threading import Thread

"""### Downloading the audio (`url = video_link`)
> - Specify here the link of the video you want to transcribe.
"""

#url = "https://www.youtube.com/watch?v=IErvIekMD3U&t=1545s&ab_channel=Mrwhosetheboss"

url="https://www.youtube.com/watch?v=0R1mA_MVItI"

ydl_opts={}

with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info_dict = ydl.extract_info(url, download=False)
video_title = info_dict['title']
video_name = "anc"
name = video_name
ydl_opts = {
     'format': 'm4a/bestaudio/best',
         'noplaylist': True,
         'continue_dl': True,
         'outtmpl': f'./{name}.wav',
         'postprocessors': [{
             'key': 'FFmpegExtractAudio',
             'preferredcodec': 'wav',
             'preferredquality': '192',
         }],
         'geobypass':True,
         'ffmpeg_location':'/usr/bin/ffmpeg'
 }

with yt_dlp.YoutubeDL(ydl_opts) as ydl:
     error_code = ydl.download(url)

"""### Spliting the audio (`min_per_split = 1`)
> - When it comes to using the free version of Google's transcriber (**`speech_recognition`**), there is a limit on the length of the video (or audio) that should not be exceeded (this limit is around 5 minutes ). To remedy this problem, the following script splits the video into one minute long intervals and puts the generated mini-videos in a directory that has a name in the form of **`split files for: Video_Name.wav`**
"""

class SplitWavAudioMubin():
    def __init__(self, folder, filename):
        self.folder = folder
        self.filename = filename
        self.filepath = folder + filename
        self.audio = AudioSegment.from_wav(self.filepath)

    def get_duration(self):
        return self.audio.duration_seconds

    def single_split(self, from_min, to_min, split_filename):
        t1 = from_min * 60 * 1000
        t2 = to_min * 60 * 1000
        split_audio = self.audio[t1:t2]
        split_audio.export(split_filename, format="wav")

    def multiple_split(self, min_per_split):
        total_mins = math.ceil(self.get_duration() / 60)
        for i in range(0, total_mins, min_per_split):
            split_fn = str(i) + '_' + self.filename
            self.single_split(i, i+min_per_split, split_fn)
            if i == total_mins - min_per_split:
                print('All splited successfully')
        print('>>> Video duration: ' + str(self.get_duration()))

def split_audio(file_name):
    directory = "splitted files for: " + file_name
    os.mkdir(directory)
    os.chdir(directory)
    split_wav = SplitWavAudioMubin("/content/", file_name)
    split_wav.multiple_split(min_per_split=1)

file_name = "{}.wav".format(video_name)
split_audio(file_name)

"""### Recognizing the text (` language = "en-US"`) https://cloud.google.com/speech-to-text/docs/languages
> - To perform text detection, we must first specify the language spoken in the video. To do this, we must search for the keyword equivalent to language in the language catalog available in the link on the title. ( In our case, it's **`English`** so the keyword is **`en-US`** )
"""

search_dir = "./"
files = filter(os.path.isfile, os.listdir(search_dir))
files = [os.path.join(search_dir, f) for f in files] # add path to each file
files.sort(key=lambda x: os.path.getmtime(x))
files

def speech_recognizer(files, frames, i):  ## This function recognizes speech in our WAV files
    texts = []
    recognizer = sr.Recognizer()

    for file in tqdm(files):
        with sr.AudioFile(file) as source:
            recorded_audio = recognizer.listen(source)
        try:
            text = recognizer.recognize_google(
                recorded_audio,
                language="en-US"  ## Replace with language keyword
            )
            texts.append(text)
        except Exception as ex:
            print(ex)
    result = ""
    for text in texts:
        result += " " + text
    frames[i] = result
    return result

def split_files(files, n_batches): ## This function split the files evenly between the threads we have.
    k, m = divmod(len(files), n_batches)
    return list(files[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n_batches))

def main(n_batches=8, verbose=True): ## By default, the maximum capacity of threads supported in collab is 8
    threads = [None]*n_batches
    frames = [None]*n_batches
    batches = split_files(files, n_batches)
    start = 0
    for i in range(len(batches)):
        if i>0:
            start_index=len(batches[i-1])
        else:
            start_index = 0
        t = Thread(target=speech_recognizer, args=(batches[i], frames, i))
        threads[i] = t
        t.start()
    for t in threads:
        t.join()
    return frames

if __name__ == "__main__":
    frames = main()

"""### Saving the recognized text
> - After performing the speech detection, we save the resulting text in a file which is in the form of **`Transcription_Video_Name.txt`**
"""

result = ""
for text in frames:
    result += " " + text

os.chdir("../")
text_file = open("Transcription_"+ file_name[:-4] +".txt", "w")
text_file.write(result)
text_file.close()

result

"""### Translating the recognized text (`dest='fr'`)
> - In addition, we have tried here to translate the text into French after transcription, using the Google API (**`googletrans`**)
> - To correctly use this API, we must replace the dest variable with the output language keyword ( In our case, **`dest='fr'`**)
"""

translator = Translator()

translate_text = ""
for text in frames:
    translate_text += " " + translator.translate(text, dest='hi').text
print(translate_text)

"""### Saving the translated text
> - We save the resulting translated text in a file which is in the form of **`Transcription_translated_Video_Name.txt`**
"""

text_file = open("Transcription_translated_"+ file_name[:-4] +".txt", "w")
text_file.write(translate_text)
text_file.close()

"""### General Idea summarization
> - Finally, we can use the text we have recovered to have a summary of the general idea discussed in the video
> - Here it is necessary to specify the **`max_length`** and the **`min_length`**, by default we have chosen that the length of the general idea of a text must be at least 10% of the total length of the text.
"""

from transformers import pipeline
summarizer = pipeline("summarization")

summarizer = pipeline("summarization")
summarized = summarizer(result, min_length=75, max_length=300)

# Print summarized text
print(summarized)

ARTICLE = result
summary_text = summarizer(ARTICLE)["summary_text"]
print(summary_text)

from transformers import BartForConditionalGeneration, BartTokenizer

def summarize_chat(messages):
    # Combine chat messages into a single string
    chat_text = " ".join(messages)

    # Load pre-trained BART model and tokenizer
    model_name = "facebook/bart-large-cnn"
    model = BartForConditionalGeneration.from_pretrained(model_name)
    tokenizer = BartTokenizer.from_pretrained(model_name)

    # Tokenize and encode the input text
    inputs = tokenizer(chat_text, return_tensors="pt", max_length=1024, truncation=True)

    # Generate summary
    summary_ids = model.generate(inputs["input_ids"], max_length=150, length_penalty=2.0, num_beams=4, early_stopping=True)
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

    return summary

result_summary = summarize_chat(translate_text)
print("\nSummary:")
x=result_summary.strip()
print(x)

