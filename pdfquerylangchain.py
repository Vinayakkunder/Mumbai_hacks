# -*- coding: utf-8 -*-
"""Copy of PdfQueryLangchain.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pNeOwlFgeidSCUlY84QDZ_3cTkOO714s

## PDF Query Using Langchain
"""

!pip install langchain
!pip install openai
!pip install faiss-cpu
!pip install tiktoken

from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS

import os
os.environ["OPENAI_API_KEY"] = "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"
os.environ["SERPAPI_API_KEY"] = "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"

# provide the path of  pdf file/files.
#pdfreader = PdfReader('/content/pythontest.txt')

# Specify the correct path to your text file
file_path = '/content/pythontest.txt'

try:
    # Open the file in read mode ('r')
    with open(file_path, 'r') as file:
        # Read the entire content of the file
        raw_test = file.read()

    # Print the content of the file
    print(raw_test)

except FileNotFoundError:
    print(f"The file '{file_path}' does not exist.")
except Exception as e:
    print(f"An error occurred: {e}")

from typing_extensions import Concatenate
# read text from pdf
raw_text = ''
for i in file_content:
    content = page.extract_text()
    if content:
        raw_text += content

raw_test

# We need to split the text using Character Text Split such that it sshould not increse token size
text_splitter = CharacterTextSplitter(
    separator = "\n",
    chunk_size = 800,
    chunk_overlap  = 200,
    length_function = len,
)
texts = text_splitter.split_text(raw_test)

len(texts)

# Download embeddings from OpenAI
embeddings = OpenAIEmbeddings()

document_search = FAISS.from_texts(texts, embeddings)

document_search

from langchain.chains.question_answering import load_qa_chain
from langchain.llms import OpenAI

chain = load_qa_chain(OpenAI(), chain_type="stuff")

query = "what is python"
docs = document_search.similarity_search(query)
chain.run(input_documents=docs, question=query)